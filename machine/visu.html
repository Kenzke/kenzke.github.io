<!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta charset="utf-8" />
    <title>Design Perception</title>
    <link rel="stylesheet" href="../css/stylez.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/aaaakshat/cm-web-fonts@latest/fonts.css">

</head>
<body>
<div id="cont">

    <div class="topl" onclick="location.href='index.html'"></div>
    <div class="topr" onclick="location.href='../epochal/index.html'"></div>


    <div class="containerlight">
        <div class="boxwhole">
            <div class="title">
                <pr>Visualizing Adversarial Attacks on Deep-Neural-Networks</pr>
            </div>
            <div class="title" style="font-size: 1.2em;padding-top:0px;margin-top:0px;">
                How can adversarial attacks on DNNs be visualized?
            </div>
        </div>
    </div>

    <br>

    <div class="containerlight">
        <div class="boxwhole">
            <img src="images/Readme.jpg" class="image" width="700px">
        </div>
    </div>


    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack" style="text-align: center;">
            Two machine learning students cross the street. 
            One of them spots a truck and says: "Look out,
            there's a deer!"
        </div>
        <br>
        <div class="textblack">
            Computer vision has made considerable progress over the last 50 years and 
            the introduction of Deep-Neural-Networks (DNNs) to the field has brought about
                a breakthrough (e.g., Krizhevsky et al., 2012).
                DNNs tasked with classifying images into categories have become increasingly 
                successful and nowadays demonstrate phenomenal accuracies (LeCun et al., 2015). 
                However, DNNs show multiple weaknesses, for example, they show remarkably little 
                generalization to image distortions, make overconfident predictions on out-of-distribution 
                data (OOD data) and their black box decisions are inscrutable. 
                Further, they are not robust against adversarial attacks, i.e., 
                input images that have been specially modified to mislead the DNN to a 
                false image-categorization (Augustin et al., 2020). In this context, adversarial attacks are used as a 
                method to make DNNs more robust. 
        </div>
        <div class="textblack">
            The project ‘Visualizing Adversarial Attacks on DNNs’ 
            was a team effort of a group of 7 students, 
            with the goal of developing a graphical interface that could visualize adversarial attacks 
            on selectable DNNs, illustrated on datasets of images. 
            The software was programmed to be used in a research context 
            in the 
            <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/maschinelles-lernen/news/" class="nav-link-text">Machine Learning Working Group</a>
            of Prof. Hein and should visualize how adversarial 
            attacks on DNNs modify, morph, and change images. The following text outlines the functionalies of the software, provides basic information on DNNs and finally draws connections to underlying philosophical questions on the nature of perception.   
            <br>
        </div>
        <br>
        <div class="subtitle">
            What does a DNN tasked with classifying images do?
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="boxwhole">
            <img src="images/DNN-01.png" class="image" width="700px">
            <div class="caption">
                <pr>Figure 1: a model of how a DNN processes an input-image into an output image-label.</pr><br>
                <pr>A DNN is made up of nodes (squares, 1 to n) that are sorted into multiple layers (a to m) and weighted connections (dotted lines) that connect nodes.</pr>
            </div>
        </div>
    </div>
    <br>
    <div class="containerlight" style="flex-direction: column;"> 
        <div class="textblack">
            In short, a DNN constructed from the architectures used in this project (e.g., ResNet50, Resnet18)
            receives a 32 by 32 pixel image as an input. 
            These input-images can be sourced from public image databases such as CIFAR_10 or CIFAR_100 
            that each contain 60,000 labelled images. 
            The task of the DNN is then to correctly label an input image.
            Figure 1 visualizes how a DNN receives an image, 
            which is processed by the first layer containing nodes from <em>a_1</em> to <em>a_n</em>. 
            Following, each subsequent layer passes information to the next layer via connections and corresponding weights.
            The term 'Architecture' describes the structure of nodes and the number of layers,
            whereas 'Model' describes the weights and connections between the nodes.
            Combining an architecture with a corresponding model results in a functioning DNN. 
            Moreover, only after training a DNN with training data and feedback, 
            can it produce accurate output labels. Training a specific architecture 
            produces a corresponding robust model with its’ weights and connections. 
            <br>
        </div>
        <div class="textblack">
            The software we developed visualized adversarial attacks on input-images.
            It presented a graphical interface that allowed a user to select models, attacks and images
            in order to execute and observe the effects of adversarial attacks on images.
            The user-interface could be divided into two sections: 
            the Model selection and the Attack selection
            <br>
        </div>
        <br>
        <div class="subtitle">
            The Model Selection
        </div>
    </div>

    <br>
    <div class="containerlight">
        <div class="boxwhole">
            <img src="images/model.png" class="image" width="700px">
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            <br>
            Here the model, architecture and dataset which should be used for the attack could be selected. 
            The selected architecture, ResNet50 in this example, is 50 layers deep. 
            The model 'At05', for example, is one corresponding and robust model for ResNet50 and it is
            the result of successful training of ResNet50 on CIFAR_100 data. 
            Some models are more robust than others, meaning they are more accurate 
            and less prone to errors even from OOD data. 
            Lastly, the dataset describes what kind of image database is used for image-categorization, 
            in this example CIFAR_100 is used, which contains images from 100 different categories.
        </div>
        <br>
        <div class="subtitle">
            The Attack Selection
        </div>
    </div>

    <div class="containerlight">
        <div class="boxwhole">
            <img src="images/attack.png" class="image" width="700px">
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            <br>
            In this section what type of attack should be used on the input images could be selected, 
            for example 'PGD' in this example. 
            Further parameters of the attack could also be defined, Epsilon, the number of Iterations, 
            Momentum, etc. Moreover, other settings such as target labels, saving of results, 
            etc. could be selected.
            <br><br>
            The compute button then executes the attack on the image datasets with the selected models.
        </div>
        <br>
        <div class="subtitle">
            Input
        </div>
    </div>

    <div class="containerlight">
        <div class="boxwhole">
            <img src="images/inputs-01.png" class="image" width="700px">
            <div class="caption">
                <pr>Figure 2: Example images from CIFAR_100, the images are labelled from top-left to bottom-right as:</pr><br>
                <pr>train, horse, cat, skyscraper, dog, muffin.</pr>
            </div>
        </div>
    </div>
    <br>
    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            The DNNs that classify images into categories, 
            receive 32 by 32 pixel images as inputs. 
            These images are either taken from image databases, 
            such as CIFAR_10 or CIFAR_100, or can be uploaded individually. 
            The CIFAR_10 database is public and contains 60,000 images from 10 different categories 
            and CIFAR_100 also contains 60,000 images but from 100 different categories. 
            CIFAR image-categories are sampled from various groups of objects, 
            for example animals, such as <em>cat</em> or <em>dog</em>, man-made objects, such as <em>lamp</em>, 
            <em>skyscraper</em> or <em>keyboard</em>, or natural objects, such as <em>cloud</em> or <em>oak</em>.
        </div>
        <br>
        <div class="subtitle">
            Output
        </div>
    </div>

    <br>
    <div class="containerlight">
        <div class="boxl">
            <img src="images/traj.gif" height="auto" class="image" width="300px">
            <br><br>
            <img src="images/comp-01.png" height="auto" class="image" width="400px">
            <div class="caption">
                <pr>Figure 3: the visualization of an adversarial attack, with the target-label lion, on an image of a train</pr><br>
            </div>
        </div>
        <div class="boxr">
            <img src="images/traj-01.png" class="image" width="600px">
            <div class="caption">
                <pr>Figure 4: the probabilities of assigned labels to the image throughout the different iterations of the attack. Train and lion are the labels with the highest probabilities at the beginning and end of the attack.</pr><br>
            </div>
        </div>
    </div>

    <br>
    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            The output shows an animation of the input image throughout multiple iterations of an attack (Figure 3). 
            The attack modifies the input image to a target image-category.
        </div>
        <div class="textblack">
            The example above demonstrates a PGD-type attack on an image of a train. 
            The target image-category of the attack is <em>lion</em>. 
            The visualization of the attack on the input image nicely demonstrates 
            how the image of a train morphs into an image of a lion. 
            Further, a plot visualizes the development of how the DNN classifies the image 
            throughout the iterations of the attack (Figure 4). 
            In this case, the DNN assigned the image the label <em>train</em> before the attack, 
            as it was the label with the highest probability. 
            At the end of the attack, which consisted of 30 iterations, 
            the attack successfully shifted the probabilities. 
            The DNN now assigned the input-image the label <em>lion</em>, as it was the label with the highest 
            probability at the end of the attack, i.e., the adversarial attack was successful. 
            It is important to note, that there are many different types of 
            adversarial attacks on DNNs and for this project 
            I specifically chose a type of attack, 
            that not only successfully shifted the probabilities of the DNNs image-labels to a target label, 
            but also visually morphed the input-image in a way that it visually resembled the target label. It shows how features of the start image were 
            adapted to resembling features of a lion. For example, the head of the train turned into visually resembling the head of the lion with its' eyes, while the 
            wheels of the train morphed into the legs of the lion.
            Other types of attacks modify images in ways in which they shift the probabilities for DNNs, 
            but when visualized on the input images do not change much to our eye.
        </div>
        <div class="textblack">
            This raises multiple questions, questions regarding the nature of perception 
            but also questions regarding human nature, the concept of knowledge and consciousness. 
            For example, when the probabilities of the labels the DNN assigned to the input image changed, 
            did the DNN <em>perceive</em> that the input image changed from a train to a lion? 
            Did the DNN perceive as a conscious entity? How is consciousness connected to perception?
            Further, how does the form of cognition and neural processing in neural structures of the brain differ from machine learning and neural network structures?
            Investigating the underlying nature of visual perception by connecting neurobiological perspectives on visual perception with machine learning approaches to visual perception and perspectives from philosophy of mind
            may reveal answers to these questions. 
            These are investigations I would like to continue in future projects under the Master Design & Computation and they carry important implications for society considering the use of machine learning
            technologies in digital environments.
        </div>
        <div class="textblack">
            When we observe how adversarial attacks modify the images above, 
            how the iterations of the attacks morph the images, 
            it becomes very clear and palpable to us how the DNN switches its’ probabilities. 
            But it should be reminded to us, 
            how this type of entity does not classify an image in the same type of way as humans do. The quality of perception supercedes simple image-classification.
        </div>
        <br>
    </div>

    <div class="containerlight" style="flex-direction: column;">
            <div class="subtitle">
                <pr>Technologies</pr>
            </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            <pr>The software was developed in Python using the Pytorch library, in a Jupyter Notebook environment. Additionally, CUDA technology allowed increased performance by parallelizing parts of computation to the GPU.</pr>
        </div>
        <br>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="subtitle">
            <pr>References</pr>
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            Augustin, M., Meinke, A., & Hein, M. (2020). Adversarial robustness on in-and out-distribution improves explainability. <em>European Conference on Computer Vision (pp. 228-245)</em>. Springer, Cham.
            <br><br>
            Krizhevsky, A., Sutskever, I. I., & Hinton, G. E. (2012). Imagenet classification with deep
            convolutional neural networks. <em>Neural Information Processing Systems (NeurIPS)</em>, 25:1097–1105.
            <br><br>
            LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. <em>nature</em>, 521(7553), 436-444.
        </div>
    </div>

    <br><br><br>

</div>

    <div class="navl" id="drop">
        <div id=curr>3.5/6</div>
        <div class="navl-content" id="content">
            <a href="../index.html">1/6</a>
            <a href="../background/index.html">2/6</a>
            <a href="index.html">3/6</a>
            <a href="visu.html" id="orange">3.5</a>
            <a href="../epochal/index.html">4/6</a>
            <a href="../feel/index.html">5/6</a>
            <a href="../conn.html">6/6</a>
        </div>
    </div>
    <!--<div class="navr">
        <a href="../index.html">Design Perception</a>
    </div>-->

    

</body>
</html>

<script src="../script.js"></script>
