<!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta charset="utf-8" />
    <title>Design Perception</title>
    <link rel="stylesheet" href="css/stylez.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/aaaakshat/cm-web-fonts@latest/fonts.css">

</head>
<body>
<div id="cont">

    <div class="topl" id="topl_id"></div>
    <div class="topr" id="topr_id"></div>

    <div class="containerlight">
        <div class="boxwhole">
            <div class="title">
                <pr>Visualizing Adversarial Attacks on Deep-Neural-Networks</pr>
            </div>
            <div class="title" style="font-size: 1.2em;padding-top:0px;margin-top:0px;">
                How can adversarial attacks on DNNs be visualized?<br>
                a technical approach
            </div>
        </div>
    </div>
    
    <div class="containerlight">
        <div class="textblack">
            Machine Learning and Deep-Neural-Networks are perhaps the most extreme examples
            of modern technologies that will have large impacts on society and the individual but are mysterious and inscrutible.
            These technologies are being used at an accelerating rate to manage data and solve complex tasks. 
            For example, Deep-Neural-Networks (DNNs) are being applied in computer vision to classify images of objects into categories, 
            which is relevant for autonomous driving. But machine learning has also spread in a much more subtle way, for example, concerning personal data analysis in the digital domain. 
            For example, machine learning is being used in various apps, such as YouTube and Instagram, to suggest personalized advertising based on behavioral data. 
            It has turned out that machine learning is incredibly successful in suggesting advertising that accurately fits the interests of the user. 
            But the dichotomy resulting from how inscrutable, yet successful the technology is may cause concern.
        </div>
    </div>
    <div class="containerlight" style="flex-direction: row; flex-wrap:wrap">
                <img src="images/trajectory_0.gif" class="neur">
                <img src="images/trajectory_1.gif" class="neur">
                <img src="images/trajectory_2.gif" class="neur">
                <img src="images/trajectory_3.gif" class="neur">
                <img src="images/trajectory_4.gif" class="neur">
                <img src="images/trajectory_5.gif" class="neur">
                <img src="images/trajectory_6.gif" class="neur">
                <img src="images/trajectory_7.gif" class="neur">
                <img src="images/trajectory_8.gif" class="neur">
                <img src="images/trajectory_9.gif" class="neur">
                <img src="images/trajectory_10.gif" class="neur">
                <img src="images/trajectory_11.gif" class="neur">
                <img src="images/trajectory_12.gif" class="neur">
                <img src="images/trajectory_13.gif" class="neur">
                <img src="images/trajectory_14.gif" class="neur">
                <img src="images/trajectory_15.gif" class="neur">
                <img src="images/trajectory_16.gif" class="neur">
                <img src="images/trajectory_17.gif" class="neur">
    </div>
    <!--<div class="containerlight">
        <div class="textblack">
            Lawmakers, politicians, and individuals are barely catching up in understanding and grasping the repercussions of tech-companies using these technologies to maximize their yields and capital. 
            Moreover, apps and digital media are spreading even faster than 5 years ago and culture is moving into the digital domain. 
            All this before we even understand what type of consequences these types of technologies will bring to us. 
            It opens up questions regarding if individuals are manipulated by AI-selected content in digital environments, 
            if this type of system will bring unfavorable repercussions for the individual human being, or if personal freedom is being exchanged for maximization of tech-company profits.
            <br>
            Worries regarding AI going rogue, turning against its' creator and taking over the world
            are unrealistic. 
            But what may be more relevant is that machine learning methods are changing society and the individual
            in ways we cannot yet foresee and with consequences that are irreversible. Conversely, it is imperative to attempt to understand the effects of these technologies.  

            <br><br>
            'Adversarial attack: <em>rose</em>', is an attempt to make computer vision tangible to the individual. 
            It shows how multiple images, representing various different image-categorious, are morphed into
            the target image-category <em>rose</em> by adverserial attacks. It visualizes how DNNs <em>perceive</em>? images and how adversarial attacks manipulate an image to maximize probabilites of a target category.
            These images show iterations of adversarial attacks on images of various image-categories and were generated by a software me and 6 other students developed as a team.
            The 
            <a href="3,5.html" class="nav-link-text">next page</a>
            provides further information on that project.
            The software was developed to be used in a research context at the Machine Learning Working Group of the University of Tübingen and used as a method to make DNNs more robust to OOD data and unfavourable environments.
            <br><br>
            This project represents how AI is slowly and subtly taking over and affecting different aspects of society. 
            Before we realize it, society may have completely changed and taken form of something completely different.
            Is this type of technology an attack on society? 
            Are adversarial attacks attacking society by supporting a type of development that we are not ready for, or is it all going the right way?
            <br><br>
            Finally, this project also draws connections to research on the nature of consciousness and the nature of visual perception. It opens up many questions such as:
            <br><br>
            What is the nature of perception? 
            <br>
            Can machinal structures develop consciousness?
            <br>
            What differentiates Deep-Neural-Networks from neural structures in biology and what is the underlying nature of consciousness and visual conscious perception?
            <br>
            ...
            
        </div>
    </div>
    -->
    <div class="containerlight" style="flex-direction: row; flex-wrap:wrap">
        <img src="images/trajectory_18.gif" class="neur">
        <img src="images/trajectory_19.gif" class="neur">
        <img src="images/trajectory_20.gif" class="neur">
        <img src="images/trajectory_21.gif" class="neur">
        <img src="images/trajectory_22.gif" class="neur">
        <img src="images/trajectory_23.gif" class="neur">
        <img src="images/trajectory_24.gif" class="neur">
        <img src="images/trajectory_25.gif" class="neur">
        <img src="images/trajectory_26.gif" class="neur">
        <img src="images/trajectory_27.gif" class="neur">
        <img src="images/trajectory_28.gif" class="neur">
        <img src="images/trajectory_29.gif" class="neur">
        <img src="images/trajectory_30.gif" class="neur">
        <img src="images/trajectory_31.gif" class="neur">
        <img src="images/trajectory_32.gif" class="neur">
        <img src="images/trajectory_33.gif" class="neur">
        <img src="images/trajectory_34.gif" class="neur">
        <img src="images/trajectory_35.gif" class="neur">
        <img src="images/trajectory_36.gif" class="neur">
        <img src="images/trajectory_37.gif" class="neur">
        <img src="images/trajectory_38.gif" class="neur">
        <img src="images/trajectory_39.gif" class="neur">
        <img src="images/trajectory_40.gif" class="neur">
        <img src="images/trajectory_41.gif" class="neur">
        <img src="images/trajectory_42.gif" class="neur">
        <img src="images/trajectory_43.gif" class="neur">
        <img src="images/trajectory_44.gif" class="neur">
        <img src="images/trajectory_45.gif" class="neur">
        <img src="images/trajectory_46.gif" class="neur">
        <img src="images/trajectory_47.gif" class="neur">
        <img src="images/trajectory_48.gif" class="neur">
        <img src="images/trajectory_49.gif" class="neur">
        <img src="images/trajectory_50.gif" class="neur">
        <img src="images/trajectory_51.gif" class="neur">
        <img src="images/trajectory_52.gif" class="neur">
        <img src="images/trajectory_53.gif" class="neur">
        <img src="images/trajectory_54.gif" class="neur">
        <img src="images/trajectory_55.gif" class="neur">
        <img src="images/trajectory_56.gif" class="neur">
        <img src="images/trajectory_57.gif" class="neur">
        <img src="images/trajectory_58.gif" class="neur">
        <img src="images/trajectory_59.gif" class="neur">
        <img src="images/trajectory_60.gif" class="neur">
        <img src="images/trajectory_61.gif" class="neur">
        <img src="images/trajectory_62.gif" class="neur">
        <img src="images/trajectory_63.gif" class="neur">
        <img src="images/trajectory_64.gif" class="neur">
        <img src="images/trajectory_65.gif" class="neur">
        <img src="images/trajectory_66.gif" class="neur">
        <img src="images/trajectory_67.gif" class="neur">
        <img src="images/trajectory_68.gif" class="neur">
        <img src="images/trajectory_69.gif" class="neur">
        <img src="images/trajectory_70.gif" class="neur">
        <img src="images/trajectory_71.gif" class="neur">
        <img src="images/trajectory_72.gif" class="neur">
        <img src="images/trajectory_73.gif" class="neur">
        <img src="images/trajectory_74.gif" class="neur">
        <img src="images/trajectory_75.gif" class="neur">
        <img src="images/trajectory_76.gif" class="neur">
        <img src="images/trajectory_77.gif" class="neur">
        <img src="images/trajectory_78.gif" class="neur">
        <img src="images/trajectory_79.gif" class="neur">
        <img src="images/trajectory_80.gif" class="neur">
        <img src="images/trajectory_81.gif" class="neur">
        <img src="images/trajectory_82.gif" class="neur">
        <img src="images/trajectory_83.gif" class="neur">
        <img src="images/trajectory_84.gif" class="neur">
        <img src="images/trajectory_85.gif" class="neur">
        <img src="images/trajectory_86.gif" class="neur">
        <img src="images/trajectory_87.gif" class="neur">
        <img src="images/trajectory_88.gif" class="neur">
        <img src="images/trajectory_89.gif" class="neur">
        <img src="images/trajectory_90.gif" class="neur">
        <img src="images/trajectory_91.gif" class="neur">
        <img src="images/trajectory_92.gif" class="neur">
        <img src="images/trajectory_93.gif" class="neur">
        <img src="images/trajectory_94.gif" class="neur">
        <img src="images/trajectory_95.gif" class="neur">
        <img src="images/trajectory_96.gif" class="neur">
        <img src="images/trajectory_97.gif" class="neur">
        <img src="images/trajectory_98.gif" class="neur">
        <img src="images/trajrose.gif" class="neur">
    </div>
    <div class="containerlight" style="flex-direction: column;">
        <div class="subtitle">
            Introduction
        </div>
        <div class="textblack">
            Computer vision has made considerable progress over the last 50 years and 
            the introduction of Deep-Neural-Networks (DNNs) to the field has brought about
                a breakthrough (e.g., Krizhevsky et al., 2012).
                DNNs tasked with classifying images into categories have become increasingly 
                successful and nowadays demonstrate phenomenal accuracies (LeCun et al., 2015). 
                However, DNNs show multiple weaknesses, for example, they show remarkably little 
                generalization to image distortions, make overconfident predictions on out-of-distribution 
                data (OOD data) and their black box decisions are inscrutable. 
                Further, they are not robust against adversarial attacks, i.e., 
                input images that have been specially modified to mislead the DNN to a 
                false image-categorization (Augustin et al., 2020). In this context, adversarial attacks are used as a 
                method to make DNNs more robust. 
        </div>
        <div class="textblack">
            The project ‘Visualizing Adversarial Attacks on DNNs’ 
            was a team effort of a group of 7 students, 
            with the goal of developing a graphical interface that could visualize adversarial attacks 
            on selectable DNNs, illustrated on datasets of images. 
            The software was programmed to be used in a research context 
            in the 
            <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/maschinelles-lernen/news/" class="nav-link-text">Machine Learning Working Group</a>
            of Prof. Hein and should visualize how adversarial 
            attacks on DNNs modify, morph, and change images. The following text outlines the functionalies of the software, provides basic information on DNNs and finally draws connections to underlying philosophical questions on the nature of perception.   
        </div>
        <div class="subtitle">
            What does a DNN tasked with classifying images do?
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="boxwhole">
            <img src="images/DNN-01.png" class="image" width="700px">
            <div class="caption">
                <pr>Figure 1: a model of how a DNN processes an input-image into an output image-label.</pr><br>
                <pr>A DNN is made up of nodes (squares, 1 to n) that are sorted into multiple layers (a to m) and weighted connections (dotted lines) that connect nodes.</pr>
            </div>
        </div>
    </div>
    <div class="containerlight" style="flex-direction: column;"> 
        <div class="textblack">
            In short, a DNN constructed from the architectures used in this project (e.g., ResNet50, Resnet18)
            receives a 32 by 32 pixel image as an input. 
            These input-images can be sourced from public image databases such as CIFAR_10 or CIFAR_100 
            that each contain 60,000 labelled images. 
            The task of the DNN is then to correctly label an input image.
            Figure 1 visualizes how a DNN receives an image, 
            which is processed by the first layer containing nodes from <em>a_1</em> to <em>a_n</em>. 
            Following, each subsequent layer passes information to the next layer via connections and corresponding weights.
            The term 'Architecture' describes the structure of nodes and the number of layers,
            whereas 'Model' describes the weights and connections between the nodes.
            Combining an architecture with a corresponding model results in a functioning DNN. 
            Moreover, only after training a DNN with training data and feedback, 
            can it produce accurate output labels. Training a specific architecture 
            produces a corresponding robust model with its’ weights and connections. 
        </div>
        <div class="textblack">
            The software we developed visualized adversarial attacks on input-images.
            It presented a graphical interface that allowed a user to select models, attacks and images
            in order to execute and observe the effects of adversarial attacks on images.
            The user-interface could be divided into two sections: 
            the Model selection and the Attack selection
        </div>
        <div class="subtitle">
            The Model Selection
        </div>
    </div>

    <div class="containerlight">
        <div class="boxwhole">
            <img src="images/model.png" class="image" width="700px">
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            Here the model, architecture and dataset which should be used for the attack could be selected. 
            The selected architecture, ResNet50 in this example, is 50 layers deep. 
            The model 'At05', for example, is one corresponding and robust model for ResNet50 and it is
            the result of successful training of ResNet50 on CIFAR_100 data. 
            Some models are more robust than others, meaning they are more accurate 
            and less prone to errors even from OOD data. 
            Lastly, the dataset describes what kind of image database is used for image-categorization, 
            in this example CIFAR_100 is used, which contains images from 100 different categories.
        </div>
        <div class="subtitle">
            The Attack Selection
        </div>
    </div>

    <div class="containerlight">
        <div class="boxwhole">
            <img src="images/attack.png" class="image" width="700px">
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            In this section what type of attack should be used on the input images could be selected, 
            for example 'PGD' in this example. 
            Further parameters of the attack could also be defined, Epsilon, the number of Iterations, 
            Momentum, etc. Moreover, other settings such as target labels, saving of results, 
            etc. could be selected.
            <br><br>
            The compute button then executes the attack on the image datasets with the selected models.
        </div>
        <div class="subtitle">
            Input
        </div>
    </div>

    <div class="containerlight">
        <div class="boxwhole">
            <img src="images/inputs-01.png" class="image" width="700px">
            <div class="caption">
                <pr>Figure 2: Example images from CIFAR_100, the images are labelled from top-left to bottom-right as:</pr><br>
                <pr>train, horse, cat, skyscraper, dog, muffin.</pr>
            </div>
        </div>
    </div>
    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            The DNNs that classify images into categories, 
            receive 32 by 32 pixel images as inputs. 
            These images are either taken from image databases, 
            such as CIFAR_10 or CIFAR_100, or can be uploaded individually. 
            The CIFAR_10 database is public and contains 60,000 images from 10 different categories 
            and CIFAR_100 also contains 60,000 images but from 100 different categories. 
            CIFAR image-categories are sampled from various groups of objects, 
            for example animals, such as <em>cat</em> or <em>dog</em>, man-made objects, such as <em>lamp</em>, 
            <em>skyscraper</em> or <em>keyboard</em>, or natural objects, such as <em>cloud</em> or <em>oak</em>.
        </div>
        <div class="subtitle">
            Output
        </div>
    </div>

    <div class="containerlight">
        <div class="boxl">
            <img src="images/traj.gif" height="auto" class="image" width="300px">
            <br><br>
            <img src="images/comp-01.png" height="auto" class="image" width="400px">
            <div class="caption">
                <pr>Figure 3: the visualization of an adversarial attack, with the target-label lion, on an image of a train</pr><br>
            </div>
        </div>
        <div class="boxr">
            <img src="images/traj-01.png" class="image" width="600px">
            <div class="caption">
                <pr>Figure 4: the probabilities of assigned labels to the image throughout the different iterations of the attack. Train and lion are the labels with the highest probabilities at the beginning and end of the attack.</pr><br>
            </div>
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            The output shows an animation of the input image throughout multiple iterations of an attack (Figure 3). 
            The attack modifies the input image to a target image-category.
        </div>
        <div class="textblack">
            The example above demonstrates a PGD-type attack on an image of a train. 
            The target image-category of the attack is <em>lion</em>. 
            The visualization of the attack on the input image nicely demonstrates 
            how the image of a train morphs into an image of a lion. 
            Further, a plot visualizes the development of how the DNN classifies the image 
            throughout the iterations of the attack (Figure 4). 
            In this case, the DNN assigned the image the label <em>train</em> before the attack, 
            as it was the label with the highest probability. 
            At the end of the attack, which consisted of 30 iterations, 
            the attack successfully shifted the probabilities. 
            The DNN now assigned the input-image the label <em>lion</em>, as it was the label with the highest 
            probability at the end of the attack, i.e., the adversarial attack was successful. 
            It is important to note, that there are many different types of 
            adversarial attacks on DNNs and for this project 
            I specifically chose a type of attack, 
            that not only successfully shifted the probabilities of the DNNs image-labels to a target label, 
            but also visually morphed the input-image in a way that it visually resembled the target label. It shows how features of the start image were 
            adapted to resembling features of a lion. For example, the head of the train turned into visually resembling the head of the lion with its' eyes, while the 
            wheels of the train morphed into the legs of the lion.
            Other types of attacks modify images in ways in which they shift the probabilities for DNNs, 
            but when visualized on the input images do not change much to our eye.
        </div>
        <div class="textblack">
            This raises multiple questions, questions regarding the nature of perception 
            but also questions regarding the difference between human perception and machine perception, 
            and the fundamental nature of consciousness.
            For example, when the probabilities of the labels the DNN assigned to the input image changed, 
            did the DNN <em>perceive</em> that the input image changed from a train to a lion? 
            Did the DNN perceive as a conscious entity? How is consciousness connected to perception?
            Further, how does the form of cognition and neural processing in neural structures of the brain differ from machine learning and neural network structures?
        </div>
        <div class="textblack">
            When we observe how adversarial attacks modify the images above, 
            how the iterations of the attacks morph the images, 
            it becomes very clear and palpable to us how the DNN switches its’ probabilities. 
            But it should be reminded to us, how this type of entity does not classify an image in the same type of way as humans do. 
            It appears that large parts of image perception in the brain are not solely
            tasked with image-classification and produce conscious percepts, at least as by-products.
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
            <div class="subtitle">
                <pr>Technologies</pr>
            </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            <pr>The software was developed in Python using the Pytorch library, in a Jupyter Notebook environment. Additionally, CUDA technology allowed increased performance by parallelizing parts of computation to the GPU.</pr>
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="subtitle">
            <pr>References</pr>
        </div>
    </div>

    <div class="containerlight" style="flex-direction: column;">
        <div class="textblack">
            Augustin, M., Meinke, A., & Hein, M. (2020). Adversarial robustness on in-and out-distribution improves explainability. <em>European Conference on Computer Vision (pp. 228-245)</em>. Springer, Cham.
            <br><br>
            Krizhevsky, A., Sutskever, I. I., & Hinton, G. E. (2012). Imagenet classification with deep
            convolutional neural networks. <em>Neural Information Processing Systems (NeurIPS)</em>, 25:1097–1105.
            <br><br>
            LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. <em>nature</em>, 521(7553), 436-444.
        </div>
    </div>
    <br>
</div>

    <div class="navl" id="drop">
        <div id=curr>3/6</div>
        <div class="navl-content" id="content">
            <a href="index.html">1/6</a>
            <a href="2.html">2/6</a>
            <a href="3.html" id="orange">3/6</a>
            <a href="4.html">4/6</a>
            <a href="5.html">5/6</a>
            <a href="6.html">6/6</a>
            <a href="7.html">7/6</a>
            <a href="8.html">8/6</a>

        </div>
    </div>
    <!--<div class="navr">
        <a href="../index.html">Design Perception</a>
    </div>-->
    <div class="bot">
        <a href="mailto: maxkenzomolitor@gmail.com" class="bot">maxkenzomolitor@gmail.com</a><br>
    </div>

    
</body>
</html>

<script src="script.js"></script>
